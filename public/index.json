[{"categories":["machine learning","Linear Algebra","Math"],"content":"Singular value decomposition tutorial","date":"2024-08-15","objectID":"/20240815_svd/","tags":["machine learning","svd","singular value decomposition","linear algebra"],"title":"Gentle Introduction to Singular Value Decomposition","uri":"/20240815_svd/"},{"categories":["machine learning","Linear Algebra","Math"],"content":"Singular Value Decomposition In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square matrix by extending the concept to asymmetric or rectangular matrices, which cannot be diagonalized directly using eigendecomposition. The SVD aims to find the following decomposition of a real-valued matrix $A$: $$A = U\\Sigma V^T,$$ where $U$ and $V$ are orthogonal (orthonormal) matrices, and $\\Sigma$ is a diagonal matrix. The columns of $U$ are called the left singular vectors of $A$, the columns of $V$ are called the right singular vectors, and the diagonal elements of $\\Sigma$ are called the singular values. Despite its widespread applications in areas such as data compression, noise reduction, and machine learning, SVD is often perceived as challenging to grasp. Many people find the mathematical intricacies daunting, even though there are numerous tutorials and explanations available. I remember struggling to fully understand SVD during my undergraduate studies, despite spending significant time on it. The complexity often arises from the abstract nature of the concepts involved, such as the interplay between eigenvectors, eigenvalues, and matrix decompositions. However, understanding SVD is crucial for many advanced techniques in data science and engineering. For instance, if the data matrix $A$ is close to being of low rank, it is often desirable to find a low-rank matrix that approximates the original data matrix well. As we will see, the singular value decomposition of $A$ provides the best low-rank approximation of $A$. The SVD process involves finding the eigenvalues and eigenvectors of the matrices $AA^T$ and $A^TA$. Since $A$ is generally not symmetric, it does not have orthogonal eigenvectors or guaranteed real eigenvalues, which complicates the process of SVD. However, the matrix $A^TA$ is guaranteed to be symmetric, as $$(A^TA)^T=A^TA,$$ and positive semi-definite, meaning all its eigenvalues are non-negative. Symmetric matrices have real eigenvalues and orthogonal eigenvectors, simplifying the decomposition process. These properties ensure that $A^TA$ can be diagonalized using an orthogonal matrix, which is crucial for deriving the SVD. The eigenvectors of $A^TA$ form the columns of $V$. We can diagonalize $A^TA$ as follows: $$A^TA = V\\Lambda V^T = \\sum_{i=1}^{n}\\lambda_i v_iv_i^T = \\sum_{i=1}^{n}\\sigma_i^2v_iv_i^T,$$ where the singular values of $A$ are defined as $\\sigma_i = \\sqrt{\\lambda_i}$. Since $A^TA$ is a symmetric matrix, its eigenvalues are non-negative. The matrix $\\Sigma$ in the SVD is a diagonal matrix whose diagonal entries are the singular values $\\sigma_1, \\dots, \\sigma_r$, where $r$ is the rank of $A$. Note that $rank(A) = rank(A^TA)$, and these singular values appear in the first $r$ positions on the diagonal of $\\Sigma$. For the $i$-th eigenvector-eigenvalue pair, we have $$A^TAv_i = \\sigma_i^2v_i.$$ Now, let’s derive the eigenvectors of $U$: \\begin{align*} A A^T (Av_i) \u0026= A (\\lambda_i v_i)\\\\ \u0026= \\lambda_i (A v_i). \\end{align*} Thus, $Av_i$ is an eigenvector of $AA^T$. However, to ensure that the matrix $U$ is orthonormal, we need to normalize these vectors as follows: \\begin{align*} u_i \u0026= \\frac{Av_i}{\\lVert Av_i\\rVert} \\\\ \u0026= \\frac{Av_i}{\\sqrt{(Av_i)^TAv_i}} \\\\ \u0026= \\frac{Av_i}{\\sqrt{v_i^TA^TAv_i}} \\\\ \u0026= \\frac{Av_i}{\\sqrt{v_i^T\\lambda_i v_i}} \\\\ \u0026= \\frac{Av_i}{\\sigma_i \\underbrace{\\lVert v_i\\rVert}_{=1}} \\\\ \u0026= \\frac{Av_i}{\\sigma_i}. \\end{align*} We can express $U$ as follows: $$U= \\left[\\frac{Av_1}{\\sigma_1}, \\dots, \\frac{Av_r}{\\sigma_r}, \\dots, \\frac{Av_n}{\\sigma_n}\\right].$$ Then, we have $$U\\Sigma = AV.$$ By rearranging, we get $$A = U\\Sigma V^{-1}.$$ Since the inverse of an orthogonal matrix $V$ is its transpose, $V^T$, the final form of the SVD is: $$A = U\\Sigma V^T.$$ ","date":"2024-08-15","objectID":"/20240815_svd/:0:0","tags":["machine learning","svd","singular value decomposition","linear algebra"],"title":"Gentle Introduction to Singular Value Decomposition","uri":"/20240815_svd/"},{"categories":["machine learning"],"content":"Introduction to Regression: Recursive Least Squares (Part 3)","date":"2024-08-12","objectID":"/20240812_recursive_least_square/","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression (Part 3)","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Deep Dive into Regression: Recursive Least Squares Explained (Part 3) ","date":"2024-08-12","objectID":"/20240812_recursive_least_square/:0:0","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression (Part 3)","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Introduction to Recursive Least Squares Ordinary least squares assumes that all data is available at once, but in practice, this isn’t always the case. Often, measurements are obtained sequentially, and we need to update our estimates as new data comes in. Simply augmenting the data matrix $\\mathbf{X}$ each time a new measurement arrives can become computationally expensive, especially when dealing with a large number of measurements. This is where Recursive Least Squares (RLS) comes into play. RLS allows us to update our estimates efficiently as new measurements are obtained, without having to recompute everything from scratch. Suppose we have an estimate $\\boldsymbol{\\theta}_{k-1}$ after $k-1$ measurements and we receive a new measurement $\\mathbf{y}_k$. We want to update our estimate $\\boldsymbol{\\theta}_k$ using the following linear recursive model: \\begin{align*} \\mathbf{y}_k\u0026=\\mathbf{X}_k\\boldsymbol{\\theta} + \\boldsymbol{\\eta}_k\\\\ \\boldsymbol{\\theta}_k\u0026=\\boldsymbol{\\theta}_{k-1} + K_k (\\mathbf{y}_k - \\mathbf{X}_k\\boldsymbol{\\theta}_{k-1}) \\end{align*} where $\\mathbf{X}_k$ is the observation matrix, $K_k$ is the gain matrix, and $\\boldsymbol{\\eta}_k$ represents the measurement error. The term $(\\mathbf{y}_k - \\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})$ is the correction term that adjusts our previous estimate using the new data. Also, $\\boldsymbol{\\eta}_k$ is the measurement error. The new estimate is modified from the previous estimate $\\boldsymbol{\\theta}_{k-1}$ with a correction via the gain matrix. To update the estimate optimally, we need to calculate the gain matrix $K_k$. This involves minimizing the variance of the estimation errors at time $k$. The error at step $k$ can be expressed as: \\begin{align*} \\boldsymbol{\\epsilon}_k \u0026= \\boldsymbol{\\theta}-\\boldsymbol{\\theta}_k \\\\ \u0026= \\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{k-1} - K_k (\\mathbf{y}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})\\\\ \u0026= \\boldsymbol{\\epsilon}_{k-1}-K_k (\\mathbf{X}_k\\boldsymbol{\\theta}+\\boldsymbol{\\eta}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})\\\\ \u0026= \\boldsymbol{\\epsilon}_{k-1}-K_k \\mathbf{X}_k(\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_{k-1})-K_k\\boldsymbol{\\eta}_k\\\\ \u0026= (I-K_k \\mathbf{X}_k)\\boldsymbol{\\epsilon}_{k-1}-K_k\\boldsymbol{\\eta}_k, \\end{align*} where $I$ is the $d\\times d$ identity matrix. The mean of this error is then \\begin{align*} \\mathbb{E}[\\boldsymbol{\\epsilon}_{k}] = (I-K_k \\mathbf{X}_k)\\mathbb{E}[\\boldsymbol{\\epsilon}_{k-1}]-K_k\\mathbb{E}[\\boldsymbol{\\eta}_{k}] \\end{align*} If $\\mathbb{E}[\\boldsymbol{\\eta}_{k}]=0$ and $\\mathbb{E}[\\boldsymbol{\\epsilon}_{k-1}]=0$, then $\\mathbb{E}[\\boldsymbol{\\epsilon}_{k}]=0$. So if the measurement noise has zero mean for all $k$, and the initial estimate of $\\boldsymbol{\\theta}$ is set equal to its expected value, then $\\boldsymbol{\\theta}_k=\\boldsymbol{\\theta}_k, \\forall k$. This property tells us that the estimator $\\boldsymbol{\\theta}_k = \\boldsymbol{\\theta}_{k-1}+K_k (\\mathbf{y}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})$ is unbiased. This property holds regardless of the value of the gain vector $K_k$. This means the estimate will be equal to the true value $\\boldsymbol{\\theta}$ on average. The key task is to find the optimal $K_k$ by minimizing the trace of the estimation-error covariance matrix $P_k = \\mathbb{E}[\\boldsymbol{\\epsilon}_k \\boldsymbol{\\epsilon}_k^T]$. This optimization leads to the following expression for ( K_k ): \\begin{align*} J_k \u0026= \\mathbb{E}[\\lVert\\boldsymbol{\\theta}-\\boldsymbol{\\theta}_k\\rVert^2]\\\\ \u0026= \\mathbb{E}[\\boldsymbol{\\epsilon}_{k}^T\\boldsymbol{\\epsilon}_{k}]\\\\ \u0026= \\mathbb{E}[tr(\\boldsymbol{\\epsilon}_{k}\\boldsymbol{\\epsilon}_{k}^T)]\\\\ \u0026= tr(P_k), \\end{align*} where $P_k=\\mathbb{E}[\\boldsymbol{\\epsilon}_{k}\\boldsymbol{\\epsilon}_{k}^T]$ is the estimation-error covariance (i.e., covariance matrix). Note that the third line holds by the trace of a product (i.e., cyclic property) and the expectation in the third line can go into the trace operator by its linearity. Next, we can obtain ","date":"2024-08-12","objectID":"/20240812_recursive_least_square/:1:0","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression (Part 3)","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Alternative Formulations Sometimes alternate forms of the equations for $P_k$ and $K_k$ are useful for computational purposes. Let’s first set $\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T+R_k = S_k$, then we get $$K_k = P_{k-1}\\mathbf{X}_k^TS_k^{-1}.$$ By putting this into $P_k$, we can obtain \\begin{align*} P_k \u0026= (I-P_{k-1}\\mathbf{X}_k^TS_k^{-1} \\mathbf{X}_k)P_{k-1}(I-P_{k-1}\\mathbf{X}_k^TS_k^{-1} \\mathbf{X}_k)^T+P_{k-1}\\mathbf{X}_k^TS_k^{-1} R_k S_k^{-1}\\mathbf{X}_kP_{k-1}\\\\ \u0026\\quad \\vdots\\\\ \u0026= P_{k-1}-P_{k-1}\\mathbf{X}_k^TS_k^{-1}\\mathbf{X}_k^TP_{k-1}\\\\ \u0026= (I-K_k\\mathbf{X}_k)P_{k-1}. \\end{align*} Note that $P_k$ is symmetric (c.f., $P_k=\\boldsymbol{\\epsilon}_{k}\\boldsymbol{\\epsilon}_{k}^T$), since it is a covariance matrix, and so is $S_k$. Then, we take the inverse of both sides of $$P_{k-1}^{-1} = \\bigg(\\underbrace{P_{k-1}}_{A}-\\underbrace{P_{k-1}\\mathbf{X}_k^T}_{B}\\big(\\underbrace{\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T}_{D}\\big)^{-1}\\underbrace{\\mathbf{X}_kP_{k-1}}_{C}\\bigg)^{-1}.$$ Next, we apply the matrix inversion lemma which is known as Sherman-Morrison-Woodbury matrix identity (or matrix inversion lemma) identity: $$(A-BD^{-1}C)^{-1} = A^{-1}+A^{-1}B(D-CA^{-1}B)^{-1}CA^{-1}.$$ Then, rewrite $P_k^{-1}$ as follows: \\begin{align*} P_k^{-1} \u0026= P_{k-1}^{-1}+P_{k-1}^{-1}P_{k-1}\\mathbf{X}_k^T\\big((\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T+R_k)-\\mathbf{X}_kP_{k-1}P_{k-1}^{-1}(P_{k-1}\\mathbf{X}_k^T)\\big)^{-1}\\mathbf{X}_kP_{k-1}P_{k-1}^{-1}\\\\ \u0026= P_{k-1}^{-1}+\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k \\end{align*} This yields an alternative expression for the covariance matrix: \\begin{align*} P_k = \\big(P_{k-1}^{-1}+\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k\\big)^{-1} \\end{align*} We can also obtain \\begin{align*} K_k = P_{k}\\mathbf{X}_k^TR_{k}^{-1} \\end{align*} By \\begin{align*} P_k \u0026= (I-K_k\\mathbf{X}_k)P_{k-1}\\\\ P_kP_{k-1}^{-1} \u0026= (I-K_k\\mathbf{X}_k)\\\\ P_kP_k^{-1} \u0026= P_kP_{k-1}^{-1}+P_k\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k=I\\\\ I \u0026= (I-K_k\\mathbf{X}_k)+P_k\\mathbf{X}_k^TR_{k}^{-1}\\mathbf{X}_k\\\\ K_k \u0026= P_{k}\\mathbf{X}_k^TR_{k}^{-1}. \\end{align*} ","date":"2024-08-12","objectID":"/20240812_recursive_least_square/:1:1","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression (Part 3)","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Summary of RLS To summarize, the RLS algorithm can be updated as follows: Gain Matrix Update: $K_k = P_{k-1}\\mathbf{X}_k^T(\\mathbf{X}_kP_{k-1}\\mathbf{X}_k^T+R_k)^{-1}$ or $K_k = P_{k}\\mathbf{X}_k^TR_{k}^{-1}$ Estimate Update: $\\boldsymbol{\\theta}_k = \\boldsymbol{\\theta}_{k-1}+K_k (\\mathbf{y}_k-\\mathbf{X}_k\\boldsymbol{\\theta}_{k-1})$ Covariance Matrix Update: $P_k = (I-K_k\\mathbf{X}_k)P_{k-1}$. $P_k = (I-K_k \\mathbf{X}_k)P_{k-1}(I-K_k \\mathbf{X}_k)^T+K_kR_kK_k^T,$ ","date":"2024-08-12","objectID":"/20240812_recursive_least_square/:1:2","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression (Part 3)","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Alternate Derivation of RLS This chapter will be posted soon. Stay tuned for updates! References: Simon, Dan, Optimal State Estimation: Kalman, H Infinity, and Nonlinear Approaches, 2006 ","date":"2024-08-12","objectID":"/20240812_recursive_least_square/:2:0","tags":["machine learning","regression","least square","recursive least squares"],"title":"Getting Started with Regression (Part 3)","uri":"/20240812_recursive_least_square/"},{"categories":["machine learning"],"content":"Introduction to Regression: Understanding the Basics (Part 2)","date":"2024-08-11","objectID":"/20240811_regression2/","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression (Part 2)","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"An Introductory Guide (Part 2) ","date":"2024-08-11","objectID":"/20240811_regression2/:0:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression (Part 2)","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Understanding Ridge Regression In machine learning, one of the key challenges is finding the right balance between underfitting and overfitting a model. Overfitting occurs when a model is too complex and captures not only the underlying patterns in the training data but also the noise. This results in a model that performs well on the training data but poorly on new, unseen data. Underfitting, on the other hand, happens when a model is too simple to capture the underlying patterns in the data, leading to poor performance both on the training data and on new data. To address these issues, regularization techniques are often used. Regularization involves adding a penalty term to the model’s objective function, which helps control the complexity of the model and prevents it from overfitting. ","date":"2024-08-11","objectID":"/20240811_regression2/:1:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression (Part 2)","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Overdetermined and Underdetermined Problems Recall that linear regression is fundamentally an optimization problem where we aim to find the optimal parameter vector $\\boldsymbol{\\theta}_{opt}$ that minimizes the residual sum of squares between the observed data and the model’s predictions. Mathematically, this is expressed as: $$\\boldsymbol{\\theta}_{opt} = \\argmin_{\\boldsymbol{\\theta}\\in \\mathbb{R}^d}\\lVert y-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2.$$ ","date":"2024-08-11","objectID":"/20240811_regression2/:2:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression (Part 2)","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Overdetermined Systems An optimization problem is termed overdetermined when the design matrix (or data matrix) $\\mathbf{X}\\in \\mathbb{R}^{m\\times d}$ has more rows than columns, i.e., $m\u003ed$. This configuration means that there are more equations than unknowns, typically leading to a unique solution. In other words, there is more information available than the number of unknowns. The unique solution can be found using the formula: $$\\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$ The solution exists if and only if $\\mathbf{X}^T\\mathbf{X}$ is invertible, which is true when the columns of $\\mathbf{X}$ are linearly independent, meaning $\\mathbf{X}$ is full rank. ","date":"2024-08-11","objectID":"/20240811_regression2/:2:1","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression (Part 2)","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Underdetermined Systems In contrast, when $\\mathbf{X}$ is fat and short (i.e., $m\u003cd$), the problem is called underdetermined. In this scenario, there are more unknowns than equations, leading to infinitely many solutions. This occurs because the system has less information than the number of unknowns. Among these, the solution that minimizes the squared norm of the parameters is preferred. This solution is known as the minimum-norm least-squares solution. For an underdetermined linear regression problem, the objective can be written as: \\begin{align*} \\boldsymbol{\\theta} = \\argmin_{\\boldsymbol{\\theta}\\in \\mathbb{R}^d} \\lVert \\boldsymbol{\\theta}\\rVert^2, \\quad \\textrm{subject to}\\ \\mathbf{y} = \\mathbf{X}\\boldsymbol{\\theta}. \\end{align*} Here, $\\mathbf{X}\\in \\mathbb{R}^{m\\times d}, \\boldsymbol{\\theta}\\in \\mathbb{R}^d,$ and $\\mathbf{y}\\in \\mathbb{R}^m$. If the matrix has rank$(\\mathbf{X})=m$, then the linear regression problem will have a unique global minimum \\begin{align*} \\boldsymbol{\\theta} = \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}. \\end{align*} This solution is called the minimum-norm least-squares solution. The proof of this solution is given by: \\begin{align*} \\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\lambda}) = \\lVert\\boldsymbol{\\theta}\\rVert^2 + \\boldsymbol{\\lambda}^T(\\mathbf{X}\\boldsymbol{\\theta}-\\mathbf{y}), \\end{align*} where $\\boldsymbol{\\lambda}$ is a Lagrange multiplier. The solution of the constrained optimization is the stationary point of the Lagrangian. To find it, we take the derivatives with respec to $\\boldsymbol{\\lambda}$ and $\\boldsymbol{\\theta}$ and setting them to zero: \\begin{align*} \\nabla_{\\boldsymbol{\\theta}} \u0026= 2 \\boldsymbol{\\theta} + \\mathbf{X}^T\\boldsymbol{\\lambda} = 0\\\\ \\nabla_{\\boldsymbol{\\lambda}} \u0026= \\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y} = 0 \\end{align*} The first equation gives us $\\boldsymbol{\\theta} = -\\mathbf{X}^T\\boldsymbol{\\lambda}/2$. Substituting it into the second equation, and assuming that rank$(\\mathbf{X})=N$ so that $\\mathbf{X}^T\\mathbf{X}$ is invertible, we have $\\boldsymbol{\\lambda} = -2 (\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}.$ Thus, we have \\begin{align*} \\boldsymbol{\\theta} = \\mathbf{X}^T(\\mathbf{X}\\mathbf{X}^T)^{-1}\\mathbf{y}. \\end{align*} Note that $\\mathbf{X}\\mathbf{X}^T$ is often called a Gram matrix, $\\mathbf{G}$. ","date":"2024-08-11","objectID":"/20240811_regression2/:2:2","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression (Part 2)","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Regularization and Ridge Regression Regularization means that instead of seeking the model parameters by minimizing the training loss alone, we add a penalty term that encourages the parameters to behave better, effectively controlling their magnitude. Ridge regression is a widely-used regularization technique that adds a penalty proportional to the square of the magnitude of the model parameters. The objective function for ridge regression is formulated as: \\begin{align*} J(\\boldsymbol{\\theta}) = \\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2_2 + \\lambda \\lVert\\boldsymbol{\\theta}\\rVert^2_2 \\end{align*} This can be expanded as: \\begin{align*} J(\\boldsymbol{\\theta}) = (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})^T(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}) + \\lambda\\boldsymbol{\\theta}^T\\boldsymbol{\\theta} \\end{align*} Breaking it down further: \\begin{align*} J(\\boldsymbol{\\theta}) = \\mathbf{y}^T\\mathbf{y} - \\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{y} - \\mathbf{y}^T\\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\theta}^T\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} + \\lambda\\boldsymbol{\\theta}^T\\mathbf{I}\\boldsymbol{\\theta} \\end{align*} To minimize the objective function $J(\\boldsymbol{\\theta})$, we take the derivative with respect to $\\boldsymbol{\\theta}$ and set it to zero: \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} = -\\mathbf{X}^T\\mathbf{y} - \\mathbf{X}^T\\mathbf{y} + \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} + \\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta} + 2\\lambda\\mathbf{I}\\boldsymbol{\\theta} = 0 \\end{align*} Solving for $\\boldsymbol{\\theta}$, we obtain the ridge regression solution: \\begin{align*} \\boldsymbol{\\theta} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y} \\end{align*} ","date":"2024-08-11","objectID":"/20240811_regression2/:3:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression (Part 2)","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Understanding the Role of $\\lambda$ When $\\lambda$ approaches 0, the regularization term $\\lambda \\lVert\\boldsymbol{\\theta}\\rVert^2_2$ becomes negligible, making ridge regression equivalent to ordinary least squares (OLS), which can lead to overfitting if the model is too complex. If $\\lambda\\to 0$, then $\\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2_2 + \\underbrace{\\lambda \\lVert\\boldsymbol{\\theta}\\rVert^2_2}_{=0}$ As $\\lambda$ increases towards infinity, the regularization term dominates, forcing $\\boldsymbol{\\theta}$ towards zero. In this case, the solution becomes overly simplistic, effectively shrinking the model parameters to zero, which may result in underfitting. $\\lambda\\to \\infty$, then $\\underbrace{\\frac{1}{\\lambda}\\lVert\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}\\rVert^2_2}_{=0} + \\lVert\\boldsymbol{\\theta}\\rVert^2_2$ Since what we want to do is to minimize the objective function, we can divide it by $\\lambda$. Therefore, the solution will be $\\boldsymbol{\\theta}=0$, because it is the smallest value the squared function can achieve. ","date":"2024-08-11","objectID":"/20240811_regression2/:3:1","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression (Part 2)","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Spectral Decomposition and Invertibility It’s important to note that $\\mathbf{X}^T\\mathbf{X}$ is always symmetric. According to the Spectral theorem, this matrix can be decomposed as $\\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T$, where $\\mathbf{Q}$ is the eigenvector matrix, and $\\mathbf{\\Lambda}$ is the diagonal matrix of eigenvalues. This allows us to express the inverse operation in ridge regression as: \\begin{align*} \\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I} \u0026= \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T+\\lambda\\mathbf{I}\\\\ \u0026= \\mathbf{Q}\\mathbf{\\Lambda}\\mathbf{Q}^T+\\lambda\\mathbf{Q}\\mathbf{Q}^T\\\\ \u0026= \\mathbf{Q}(\\mathbf{\\Lambda}+\\lambda\\mathbf{I})\\mathbf{Q}^T. \\end{align*} Even if $\\mathbf{X}^T\\mathbf{X}$ is not invertible (or is close to being non-invertible), the regularization constant $\\lambda$ ensures invertibility by making the matrix full-rank. ","date":"2024-08-11","objectID":"/20240811_regression2/:3:2","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression (Part 2)","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Dual Form of Ridge Regression Ridge regression can also be expressed in its dual form, which is particularly useful for solving underdetermined problems: \\begin{align*} (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})\\boldsymbol{\\theta} \u0026= (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})(\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}\\\\ (\\mathbf{X}^T\\mathbf{X}+\\lambda\\mathbf{I})\\boldsymbol{\\theta} \u0026= \\mathbf{X}^T\\mathbf{y}\\\\ \\boldsymbol{\\theta} \u0026= \\lambda^{-1}\\mathbf{I}(\\mathbf{X}^T\\mathbf{y}-\\mathbf{X}^T\\mathbf{X}\\boldsymbol{\\theta})\\\\ \u0026= \\mathbf{X}^T\\lambda^{-1}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})\\\\ \u0026= \\mathbf{X}^T\\alpha\\\\ \\lambda\\alpha \u0026= (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})\\\\ \u0026= (\\mathbf{y}-\\mathbf{X}\\mathbf{X}^T\\alpha)\\\\ \\mathbf{y} \u0026= (\\mathbf{X}\\mathbf{X}^T\\alpha+\\lambda\\alpha)\\\\ \\alpha \u0026= (\\mathbf{X}\\mathbf{X}^T+\\lambda)^{-1}\\mathbf{y}\\\\ \\alpha \u0026= (\\mathbf{G}+\\lambda)^{-1}\\mathbf{y}. \\end{align*} Here, $\\alpha$ represents the dual coefficients, and $\\mathbf{G}$ is the Gram matrix. This formulation is especially powerful when dealing with high-dimensional data, where the number of features exceeds the number of samples. ","date":"2024-08-11","objectID":"/20240811_regression2/:3:3","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression (Part 2)","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Considering Varying Confidence in Measurements: Weighted Regression Up to this point, we have assumed that all measurements are equally reliable. However, in practice, the confidence in different measurements may vary. To account for this, we can consider the situation where the noise associated with each measurement has a zero mean and is independent across measurements. Under these conditions, the covariance matrix for the measurement noise can be expressed as follows: \\begin{align*} R \u0026= \\mathbb{E}(\\eta\\eta^T)\\\\ \u0026= \\begin{bmatrix} \\sigma_1^2 \u0026 \\dots \u0026 0\\\\ \\vdots \u0026 \\ddots \u0026 \\vdots\\\\ 0 \u0026 \\dots \u0026 \\sigma_l^2 \\end{bmatrix} \\end{align*} By denoting the error vector $\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}$ as $\\boldsymbol{\\epsilon} = (\\epsilon_1, \\dots, \\epsilon_l)^T$, we will minimize the sum of squared differences weighted over the variations of the measurements: \\begin{align*} J(\\tilde{\\mathbf{x}}) \u0026= \\boldsymbol{\\epsilon}^TR^{-1}\\boldsymbol{\\epsilon}=\\frac{\\boldsymbol{\\epsilon}_1^2}{\\sigma_1^2}+\\dots+\\frac{\\boldsymbol{\\epsilon}_l^2}{\\sigma_l^2}\\\\ \u0026= (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta})^TR^{-1}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\theta}) \\end{align*} Note that by dividing each residual by its variance, we effectively equalize the influence of each data point on the overall fitting process. Subsequently, by taking the partial derivative of $J$ with respect to $\\boldsymbol{\\theta}$, we get the best estimate of the parameter, which is given by $$\\boldsymbol{\\theta} = (\\mathbf{X}^TR^{-1}\\mathbf{X})^{-1}\\mathbf{X}^TR^{-1}\\mathbf{y}.$$ Note that the measurement noise matrix $R$ must be non-singular for a solution to exist. To learn more, please take a look at this note ! This article continues in Part 3. References: H. Pishro-Nik, Introduction to Probability, Statistics, and Random Processes, 2014 Simon, Dan, Optimal State Estimation: Kalman, H Infinity, and Nonlinear Approaches, 2006 ","date":"2024-08-11","objectID":"/20240811_regression2/:4:0","tags":["machine learning","regression","least square","ridge regression","weighted regression","dual form"],"title":"Getting Started with Regression (Part 2)","uri":"/20240811_regression2/"},{"categories":["machine learning"],"content":"Introduction to Regression: Understanding the Basics (Part 1)","date":"2024-08-10","objectID":"/20240810_regression1/","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression (Part 1)","uri":"/20240810_regression1/"},{"categories":["machine learning"],"content":"An Introductory Guide (Part 1) Even with the rapid advancements in deep learning, regression continues to be widely used across various fields (e.g., finance, data science, statistics, and so on), maintaining its importance as a fundamental algorithm. That’s why I’ve decided to share this post, which is the first article in a dedicated series on regression. This series is designed to provide a thorough review while offering a gentle and accessible introduction. ","date":"2024-08-10","objectID":"/20240810_regression1/:0:0","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression (Part 1)","uri":"/20240810_regression1/"},{"categories":["machine learning"],"content":"Linear Regression Regression is a method used to identify the relationship between input and output variables. In a regression problem, we are given a set of noisy measurements (or output data) $\\mathbf{y} = [y_1, \\dots, y_d]^T$, which are affected by measurement noise $\\boldsymbol{\\eta} = [\\eta_1, \\dots, \\eta_d]^T$. The corresponding input data is denoted by $\\mathbf{x} = [x_1, \\dots, x_d]$. We refer to the collection of these input-output pairs as the training data, $\\mathcal{D} = {(\\mathbf{x}_1, \\mathbf{y}_1), \\dots, (\\mathbf{x}_m, \\mathbf{y}_m)}$. The true relationship between the input and output data is unknown and is represented by a function $f(\\cdot)$ that maps $\\mathbf{x}_n$ to $y_n$, i.e., $$ \\mathbf{y} = f(\\mathbf{x}). $$ Determining the exact function $f(\\cdot)$ from a finite set of data points $\\mathcal{D}$ is not feasible because there are infinitely many possible mappings for each $\\mathbf{x}_i$. The idea of regression is to introduce structure to the problem. Instead of trying to find the true $f(\\cdot)$, we seek an approximate model $g_\\theta(\\cdot)$, which is parameterized by $\\boldsymbol{\\theta} = [\\theta_1,\\dots,\\theta_d]^T$. For example, we might assume a linear relationship between $(\\mathbf{x}_n, \\mathbf{y}_n)$: \\begin{align*} g_{\\boldsymbol{\\theta}}(\\mathbf{y}) = \\mathbf{X}\\boldsymbol{\\theta} + \\boldsymbol{\\eta}, \\end{align*} where $\\mathbf{X}$ is an $m \\times d$ input matrix derived from our observations. Since the true relationship is unknown, any chosen model is essentially a hypothesis. However, we can quantify the error in our model. Given a parameter $\\boldsymbol{\\theta}$, the error between the noisy measurements and the estimated values is: \\begin{align*} \\boldsymbol{\\epsilon} = \\mathbf{y} - \\mathbf{X}\\boldsymbol{\\theta}. \\end{align*} The goal of regression is to find the best $\\boldsymbol{\\theta}$ that minimizes this error. This leads us to the following objective function: \\begin{align*} J(\\boldsymbol{\\theta}) = \\boldsymbol{\\epsilon}^T \\boldsymbol{\\epsilon}. \\end{align*} This objective function is equivalent to minimizing the mean squared error (MSE): \\begin{align*} MSE = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\mathbf{x}_i \\boldsymbol{\\theta})^2. \\end{align*} We can optimize this function in closed form as follows: \\begin{align*} J(\\boldsymbol{\\theta}) \u0026= \\lVert\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta}\\rVert^2_2 \\\\ \u0026= (\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta})^T(\\mathbf{y} - \\mathbf{X} \\boldsymbol{\\theta}) \\\\ \u0026= \\mathbf{y}^T \\mathbf{y} - \\boldsymbol{\\theta}^T \\mathbf{X}^T \\mathbf{y} - \\mathbf{y}^T \\mathbf{X} \\boldsymbol{\\theta} + \\boldsymbol{\\theta}^T \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\theta}. \\end{align*} To find the $\\boldsymbol{\\theta}$ that minimizes the objective function, we compute the derivative of the function and set it equal to zero: \\begin{align*} \\frac{\\partial J}{\\partial \\boldsymbol{\\theta}} = -\\mathbf{X}^T \\mathbf{y} - \\mathbf{X}^T \\mathbf{y} + \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\theta} + \\mathbf{X}^T \\mathbf{X} \\boldsymbol{\\theta} = 0, \\end{align*} which simplifies to: \\begin{align*} \\mathbf{X}^T (\\mathbf{X} \\boldsymbol{\\theta} - \\mathbf{y}) = 0, \\end{align*} leading to the solution: \\begin{align*} \\boldsymbol{\\theta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}. \\end{align*} The equation $\\mathbf{X}^T(\\mathbf{X} \\boldsymbol{\\theta} - \\mathbf{y}) = 0$ is known as the normal equation. ","date":"2024-08-10","objectID":"/20240810_regression1/:1:0","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression (Part 1)","uri":"/20240810_regression1/"},{"categories":["machine learning"],"content":"Python Code Let’s implement a simple regression in Python: import numpy as np import matplotlib.pyplot as plt N = 50 x = np.random.randn(N) w_1 = 3.4 # True Parameter w_0 = 0.9 # True Parameter y = w_1*x + w_0 + 0.3*np.random.randn(N) # Synthesize training data X = np.column_stack((x, np.ones(N))) W = np.array([w_1, w_0]) # From Scratch XtX = np.dot(X.T, X) XtXinvX = np.dot(np.linalg.inv(XtX), X.T) # d x m W_best = np.dot(XtXinvX, y.T) print(f\"W_best: {W_best}\") # Pythonic Approach theta = np.linalg.lstsq(X, y, rcond=None)[0] print(f\"Theta: {theta}\") t = np.linspace(0, 1, 200) y_pred = W_best[0]*t+W_best[1] yhat = theta[0]*t+theta[1] plt.plot(x, y, 'o') plt.plot(t, y_pred, 'r', linewidth=4) plt.show() To learn more, please take a look at this note ! This article continues in Part 2 . References: H. Pishro-Nik, Introduction to Probability, Statistics, and Random Processes, 2014 ","date":"2024-08-10","objectID":"/20240810_regression1/:1:1","tags":["machine learning","regression","least square"],"title":"Getting Started with Regression (Part 1)","uri":"/20240810_regression1/"},{"categories":["gpg"],"content":"Guide to encrypt/decrypt data using GPG","date":"2024-08-04","objectID":"/20240804_gpg_encryption/","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"Securing Your Privacy The importance of securing your data has become critical in the modern digital era. This post explores a versatile tool called GnuPG, or GNU Privacy Guard, which allows you to encrypt your data and communications, ensuring that only the intended recipients can access them. ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:0:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"Asymmetric Encryption Before looking at GPG, let’s first review some encryption approaches. A very naive approach to sharing encrypted files is to use the same secret key between a sender and a receiver. This approach is known as symmetric encryption. However, the symmetric approach has a limitation in that it requires a secure method for key exchange. To address this issue, asymmetric encryption is preferred. Asymmetric encryption, also known as public-key cryptography, is a method of encryption that uses a pair of keys: a public key and a private key, to encrypt and decrypt data. The public key is used for encryption. It is openly shared and can be distributed to anyone, allowing anyone to encrypt a message intended for the key owner. The private key is used for decryption. It is kept secret and known only to the owner, allowing the key owner to decrypt messages that were encrypted with the corresponding public key. This is how asymmetric encryption works: Key Pair Generation: A user generates a pair of keys: one public and one private. The public key is shared widely, while the private key is kept secure. Encryption: When someone wants to send a secure message, they use the recipient’s public key to encrypt the message. This ensures that only the recipient, who has the corresponding private key, can decrypt and read the message. Decryption: The recipient uses their private key to decrypt the received message. The private key is the only key that can decrypt the message encrypted with the corresponding public key. ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:1:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"How to Use GPG? Installation: sudo apt-get install gnupg # Ubuntu sudo pacman -S gnupg # Arch Generate a GPG Key: gpg --full-gen-key Export Public Key: gpg --export --armor your-email@example.com \u003e publickey.asc Import Public Key: gpg --import publickey.asc Encrypt a File: gpg --output encryptedfile.gpg --encrypt --recipient recipient@example.com file.txt Decrypt a File: gpg --output file.txt --decrypt encryptedfile.gpg ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:2:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"Singinig/Verifying Files A detached signature is a separate file that contains the signature of the original file. gpg --output file.sig --detach-sign file.txt file.txt is the file you want to sign. file.sig is the signature file generated. To verify the detached signature: gpg --verify file.sig file.txt To sign text files, gpg --clearsign \u003cfile name\u003e This will create a file called file.txt.asc which contains the original text and the signature. To verify gpg --verify \u003cfile.asc\u003e ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:3:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"A Simple GitHub Verification using ssh with gpg ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:4:0","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["gpg"],"content":"Generate SSH key ssh-keygen -t rsa or simply ssh-keygen id_rsa : private key id_rsa.pub : public key Go to SSH and GPG keys in the setting menu of GitHub Just paste your public key ","date":"2024-08-04","objectID":"/20240804_gpg_encryption/:4:1","tags":["gpg","encryption","decryption","privacy"],"title":"Data Encryption using GPG!","uri":"/20240804_gpg_encryption/"},{"categories":["Tools"],"content":"Guide to manage your tasks using TaskSpooler","date":"2024-07-13","objectID":"/20240713_taskspooler/","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"What is TaskSpooler ? TaskSpooler (ts) is a lightweight job scheduler that allows you to queue up your tasks and execute them in order. It’s particularly useful for environments where tasks need to be managed sequentially or with a controlled degree of parallelism. Unlike more complex systems like SLURM, TaskSpooler is designed for simplicity and ease of use, making it accessible for individual researchers and small teams. ","date":"2024-07-13","objectID":"/20240713_taskspooler/:0:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"Efficient Job Scheduling for ML/DL Researchers with Taskspooler In the dynamic field of Machine Learning (ML) and Deep Learning (DL), managing and optimizing computational resources is crucial. For researchers frequently running numerous experiments, an efficient job scheduler can be a game-changer. Enter Taskspooler, a powerful yet user-friendly job scheduler for Linux, designed to help you manage and schedule your jobs in a queue. Taskspooler is a simpler alternative to SLURM, providing many benefits for ML/DL researchers, especially when it comes to utilizing GPUs efficiently. ","date":"2024-07-13","objectID":"/20240713_taskspooler/:1:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"TL;DR Job Queuing: Easily queue up multiple jobs, specifying the order of execution. Resource Management: Find and allocate empty GPUs to your tasks, maximizing resource utilization. Monitoring: Track the status of your jobs in real-time. Simplicity: A straightforward command-line interface that requires minimal setup and configuration. Parallel Execution: Control the number of jobs running simultaneously, which is essential for managing GPU workloads effectively. ","date":"2024-07-13","objectID":"/20240713_taskspooler/:1:1","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"Dive into TaskSpooler ","date":"2024-07-13","objectID":"/20240713_taskspooler/:2:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"Installation First, clone the repository: git clone https://github.com/justanhduc/task-spooler Install GPU Version To set up Task Spooler with GPU support, run the provided script: ./install_make Alternatively, to use CMake: ./install_cmake If Task Spooler is already installed, and you want to reinstall or upgrade, run: ./reinstall Install CPU Version If you would like to install only the CPU version, use the following commands (recommended): make cpu sudo make install or via CMake: mkdir build \u0026\u0026 cd build cmake .. -DTASK_SPOOLER_COMPILE_CUDA=OFF -DCMAKE_BUILD_TYPE=Release make sudo make install ","date":"2024-07-13","objectID":"/20240713_taskspooler/:2:1","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"Basic Usage Let’s first put your task into a queue: ts python main.py This command queues up your python main.py. Keeping track of running and pending jobs is crucial for optimizing your workflow. Taskspooler provides real-time updates on job status, allowing you to make informed decisions and adjustments on the fly. To check the overall queue status, simply type: ts This returns your jobs with the job ID, state, time, and the command you typed. To track the status of your jobs: ts -c \u003cjob-id\u003e You can delete finished jobs in the job list by: ts -C To set the size of your job queue (i.e., to limit/expand the number of parallel running processes): ts -S \u003cqueue-size\u003e ","date":"2024-07-13","objectID":"/20240713_taskspooler/:3:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"GPU Utilization For ML/DL researchers, GPUs are invaluable but often limited resources. Taskspooler helps you find available GPUs and assign tasks to them efficiently. This ensures that your experiments run smoothly without unnecessary delays due to resource contention. To specify GPU indices for a job without checking whether they are free, use: ts -g [id,...] python main.py For instance: ts -g 1 python main.py This allows you to run your model on GPU 1. To get the GPU usage: ts -g ","date":"2024-07-13","objectID":"/20240713_taskspooler/:4:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Tools"],"content":"Conclusion Taskspooler offers a simple yet powerful solution for job scheduling and resource management, making it an excellent tool for ML/DL researchers. By effectively queuing your tasks and optimizing GPU usage, you can streamline your workflow and focus more on the research itself rather than managing computational resources. Whether you’re working on a single machine or a small cluster, Taskspooler can significantly enhance your productivity and efficiency. If you want to know more visit its official github repo Happy experimenting! ","date":"2024-07-13","objectID":"/20240713_taskspooler/:5:0","tags":["taskspooler","scheduler","task management"],"title":"Manage your task with TaskSpooler!","uri":"/20240713_taskspooler/"},{"categories":["Python"],"content":"Guide to keep manage dependencies in Python","date":"2024-07-07","objectID":"/20240707_poetry/","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Introduction Poetry is a dependency management and packaging tool in Python, aiming to improve how you define, install, and manage project dependencies. Installation: You can install Poetry through its custom installer script or using package managers. The recommended way is to use their installer script to ensure you get the latest version. Creating a New Project: Use poetry new \u003cproject-name\u003e to create a new project with a standard layout. Adding Dependencies: Add new dependencies directly to your project using poetry add \u003cpackage\u003e. Poetry will resolve the dependencies and update the pyproject.toml and poetry.lock files. Installing Dependencies: Running poetry install in your project directory will install all dependencies defined in your pyproject.toml file. ","date":"2024-07-07","objectID":"/20240707_poetry/:0:0","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Poetry Example ","date":"2024-07-07","objectID":"/20240707_poetry/:1:0","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Setting Up a New Project To create a new project named example_project with Poetry and manage its dependencies: poetry new example_project This command creates a new directory named example_project with some initial files, including a pyproject.toml file for configuration. The pyproject.toml file is what is the most important here. This will orchestrate your project and its dependencies. For now, it looks like this: [tool.poetry] name = \"poetry-demo\" version = \"0.1.0\" description = \"\" authors = [\"author \u003cauthor@xxxxx.xxx\u003e\"] readme = \"README.md\" packages = [{include = \"poetry_demo\"}] [tool.poetry.dependencies] python = \"^3.7\" [build-system] requires = [\"poetry-core\"] build-backend = \"poetry.core.masonry.api\" we are allowing any version of Python 3 that is greater than 3.7.0. If you want to use Poetry only for dependency management but not for packaging, you can use the non-package modei: [tool.poetry] package-mode = false ","date":"2024-07-07","objectID":"/20240707_poetry/:1:1","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Add and Install Dependencies Suppose your project depends on requests for making HTTP requests and pytest for testing. To add these: poetry add requests poetry add pytest --dev The --dev flag indicates that pytest is a development dependency, not required for production. To remove a package, poetry remove \u003cpackage\u003e Running the command below will install all dependencies listed in your pyproject.toml: poetry install This also creates a virtual environment for your project if it doesn’t already exist. ","date":"2024-07-07","objectID":"/20240707_poetry/:1:2","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Run Your Project Run directly To run a script or start your project within the Poetry-managed virtual environment: poetry run python my_script.py This command ensures that python and any other commands are run within the context of your project’s virtual environment, using the correct versions of Python and all dependencies. Entry Point In Poetry, an entry point is a way to specify which Python script should be executed when your package is run. This is particularly useful for creating command-line applications or defining executable scripts that can be run directly from the command line after your package is installed. To define an entry point in a Poetry project, you use the [tool.poetry.scripts] section in your pyproject.toml file. This section allows you to map a command name to a Python function, which will be executed when the command is run. Edit your pyproject.toml file to include the [tool.poetry.scripts] section. This is where you define your entry point. Add the following lines to the pyproject.toml file: [tool.poetry.scripts] greet-app = \"greet_app.cli:main\" Here, greet-app is the command name, and greet_app.cli:main specifies the main function in the cli.py module inside the greet_app package. Shell Using Virtual Environment Shell: If you frequently run commands, you might find it convenient to activate the Poetry-managed virtual environment shell: poetry shell This will activate the virtual environment, and you can run your command without prefixing it with poetry run: greet-app To exit this new shell type exit. To deactivate the virtual environment without leaving the shell use deactivate. ","date":"2024-07-07","objectID":"/20240707_poetry/:1:3","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Init Method Instead of creating a new project, Poetry can be used to ‘initialise’ a pre-populated directory. To interactively create a pyproject.toml file in directory pre-existing-project: poetry init Then, install it by poetry install To see the information about the project poetry env info -p: prints a path of the virtual environment By setting a config, you can generate a virtual environment in your projects: poetry config virtualenvs.in-project true ","date":"2024-07-07","objectID":"/20240707_poetry/:1:4","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Python"],"content":"Use with Git Poetry automatically creates a .gitignore file for you. It should include entries to ignore the virtual environment directory (.venv if you use Poetry’s built-in virtual environment management). Whenever you add or update dependencies, make sure to commit the pyproject.toml and poetry.lock files: To clone your project: git clone \u003cyour-repository-url\u003e cd my-project poetry install ","date":"2024-07-07","objectID":"/20240707_poetry/:2:0","tags":["Python","Poetry","Virtual Environment"],"title":"Dependency Management in Python","uri":"/20240707_poetry/"},{"categories":["Linux"],"content":"Pacman tutorial","date":"2024-05-01","objectID":"/20240501_pacman/","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Pacman , the package manager for Arch Linux, is known for its simple binary package format and easy-to-use build system . The primary aim of Pacman is to facilitate straightforward management of packages from both the official repositories and user-generated builds. Pacman ensures your system remains updated by synchronizing the package lists with the master server. This client/server model simplifies the process of downloading and installing packages, along with all their dependencies, using basic commands. ","date":"2024-05-01","objectID":"/20240501_pacman/:0:0","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Installing and Upgrading Packages Install a Package: sudo pacman -S \u003cpackage-name\u003e Full System Upgrade: sudo pacman -Syu -y synchronizes the database, similar to sudo apt-get update. -u upgrades all out-of-date packages, akin to sudo apt-get upgrade. Installing Packages from Git: Clone the package and install: git clone \u003crepository-url\u003e makepkg -si ","date":"2024-05-01","objectID":"/20240501_pacman/:0:1","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Removing Packages Remove a Specific Package: sudo pacman -R \u003cpackage-name\u003e Using -s removes dependencies not required by other packages, but be cautious as it may affect dependencies needed by other programs. Best Practice for Removing Packages: sudo pacman -Rns \u003cpackage-name\u003e Remove Obsolete Packages: sudo pacman -Sc ","date":"2024-05-01","objectID":"/20240501_pacman/:0:2","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Managing Package Lists List All Installed Packages: sudo pacman -Q List Manually Installed Packages: sudo pacman -Qe List Installed AUR Packages: sudo pacman -Qm List Unneeded Dependencies: sudo pacman -Qdt ","date":"2024-05-01","objectID":"/20240501_pacman/:0:3","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Getting Started with Pacman Creating a List of Installed Packages: Generate a list to easily reinstall packages on a new system: pacman -Qqen \u003e pkglist.txt To reinstall packages from the list: pacman -S - \u003c pkglist.txt ","date":"2024-05-01","objectID":"/20240501_pacman/:1:0","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Rollback to Previous Versions List Cached Packages: ls /var/cache/pacman/pkg/ To downgrade a package: sudo pacman -U \u003cpackage-file\u003e ","date":"2024-05-01","objectID":"/20240501_pacman/:1:1","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Managing Mirror Lists Edit and Refresh Mirror List: sudo vim /etc/pacman.d/mirrorlist sudo pacman -Syy Use -yy to force a refresh of the package databases, even if they are up to date. ","date":"2024-05-01","objectID":"/20240501_pacman/:1:2","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Linux"],"content":"Configuration Tips Enable Parallel Downloads: Open your configuration file: sudo vim /etc/pacman.conf Then uncomment or add the following line to enable multiple simultaneous downloads: ParallelDownloads=5 Ignore Specific Packages: Add the following line to /etc/pacman.conf to prevent specific packages from being updated: IgnorePkg = postgresql* ","date":"2024-05-01","objectID":"/20240501_pacman/:1:3","tags":["Arch","Linux","Pacman"],"title":"Handy Pacman Commands in Arch Linux","uri":"/20240501_pacman/"},{"categories":["Vim"],"content":"The Appeal of Vim in Modern Programming","date":"2024-05-01","objectID":"/20240501_vim/","tags":["vim"],"title":"Vim, Type at the Speed of Thought!","uri":"/20240501_vim/"},{"categories":["Vim"],"content":"While it may look somewhat obsolete in an era dominated by graphically rich IDEs, Vim remains not just a highly relevant and effective tool for today’s programmers but also a badge of coolness in the tech world. Those who master its commands are often seen as coding wizards. With its unique advantages in speed, efficiency, and customizability, Vim is an invaluable asset in software development environments, proving that old-school can still be trendy. Vim is celebrated for its minimalistic approach, using fewer system resources, which facilitates fast and responsive editing. This efficiency is further enhanced by Vim’s command-driven interface, allowing developers to perform complex edits quickly through simple keystrokes. This minimizes the need for mouse use, thereby reducing the risk of repetitive strain injuries on the wrists. Here’s an example of how to yank (copy) and paste in Vim: To yank (copy) a line in Vim, just press yy. To paste the yanked text, simply move the cursor to where you want to insert the copied text and press p to paste after the cursor position or P to paste before it. The universal Ctrl+C to copy and Ctrl+V to paste are straightforward and familiar to most users. However, this often requires alternating between the keyboard and mouse, which can slow down the editing process and increase physical strain on the wrists. Vim’s approach is designed to keep your fingers on the keyboard, reducing the need to switch to a mouse and enhancing focus and productivity. This is especially beneficial in coding and scripting environments where rapid navigation and changes are common. Beyond its capabilities as a programming editor, Vim excels as a general-purpose text editor. It’s an excellent tool for note-taking and managing documentation, thanks to its lightweight nature and fast operation. Moreover, Vim is highly effective for composing complex documents in LaTeX (See my machine learning study notes !), allowing users to edit large amounts of text with ease and precision. The ability to customize Vim with plugins and scripts also extends its functionality into areas like PDF viewing and reference management. Customizability is another cornerstone of Vim’s design. Developers can adjust nearly every aspect of the editor to fit their workflow, from key bindings to complex integrations. Vim’s adaptability extends to broader system configurations, seamlessly integrating with tools like the i3 window manager (i3wm) and file browsers like LF and Ranger. This integration allows for a more unified and efficient desktop environment, where everything from file management to window resizing is streamlined through Vim-like commands. Mastering Vim not only boosts your coding efficiency but also earns you some cool points among tech peers who appreciate the power and elegance of classic tools. I will post more Vim tips soon! ","date":"2024-05-01","objectID":"/20240501_vim/:0:0","tags":["vim"],"title":"Vim, Type at the Speed of Thought!","uri":"/20240501_vim/"},{"categories":["Python"],"content":"Why Use Python's `pdb` Debugger Over an IDE?","date":"2024-04-27","objectID":"/20240426_pdb/","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"When it comes to debugging Python code, most programmers reach for an Integrated Development Environment (IDE) because of its convenience and powerful features. However, there’s a classic, built-in tool that shouldn’t be overlooked: Python’s own debugger, pdb. This tool might seem basic at first glance, but it offers some compelling advantages, especially in scenarios where an IDE might be less effective. Here’s why you might consider using pdb for debugging your Python projects: ","date":"2024-04-27","objectID":"/20240426_pdb/:0:0","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"Simplicity pdb comes as part of Python’s standard library, which means it’s ready to use out of the box—no installation or complex setup required. If you’re working on a simple script or need a quick debugging session, pdb is just a few keystrokes away. pdb offers an interactive session that lets you control the flow of your program. You can step through your code line by line, inspect and modify variables, and execute Python commands on the fly. This hands-on control can make finding and fixing bugs much clearer and sometimes even faster. ","date":"2024-04-27","objectID":"/20240426_pdb/:0:1","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"Environment Independence One of pdb’s greatest strengths is its versatility. Whether you’re coding on a local machine, a remote server, or even in a container, pdb works just the same. This universal compatibility is a huge plus, particularly when dealing with production environments where installing a full-fledged IDE isn’t feasible. Also, pdb operates entirely in the terminal, it’s perfect for low-resource environments or situations where a graphical interface might slow you down. This makes pdb incredibly efficient and responsive, even over network connections like SSH. ","date":"2024-04-27","objectID":"/20240426_pdb/:0:2","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"Flexibility in Use You can start pdb in several ways: directly from the command line, by inserting a breakpoint in your code, or as a module. This flexibility allows you to adapt your debugging approach to the needs of each specific project or problem. For developers who prefer working in text editors like Vim or Emacs, pdb integrates smoothly, enabling powerful debugging without leaving your editor. This integration supports a streamlined workflow, particularly for those who favor a more textual or minimalist development environment. ","date":"2024-04-27","objectID":"/20240426_pdb/:0:3","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"Conclusion While modern IDEs are undeniably powerful and user-friendly, pdb holds its own with features that are particularly suited to debugging in a variety of environments and situations. It’s a tool that encourages mastery of debugging by getting you close to the code in a way that GUI tools sometimes can’t match. Whether you’re a beginner looking to understand the inner workings of Python or an experienced developer needing a reliable tool on a remote server, pdb is worth exploring. ","date":"2024-04-27","objectID":"/20240426_pdb/:0:4","tags":["Python","pdb","debug"],"title":"Why Use Python's `pdb` Debugger Over an IDE?","uri":"/20240426_pdb/"},{"categories":["Python"],"content":"A tutorial for Pydantic","date":"2024-04-26","objectID":"/20240426_pydantic/","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Python’s dynamic typing system is indeed convenient, allowing you to create variables without explicitly declaring their types. While this flexibility can streamline development, it can also introduce unexpected behavior, particularly when handling data from external sources like APIs or user input. Consider the following scenario: employee = Employee(\"Han\", 30) # Correct employee = Employee(\"Moon\", \"30\") # Correct Here, the second argument is intended to represent an age, typically an integer. However, in the second example, it’s a string, potentially leading to errors or unexpected behavior down the line. To address such issues, Pydantic offers a solution through data validation. Pydantic is a library specifically designed for this purpose, ensuring that the data conforms to pre-defined schemas. The primary method of defining schemas in Pydantic is through models. Models are essentially classes that inherit from pydantic.BaseModel and define fields with annotated attributes. You can think of models as similar to structs in languages like C. While Pydantic models share similarities with Python’s dataclasses, they are preferred when data validation is essential. Pydantic models guarantee that the fields of an instance will adhere to specified types, providing both runtime validation and serving as type hints during development. Let’s illustrate this with a simple example: from pydantic import BaseModel class User(BaseModel): id: int name: str = \"John Doe\" User model has two fields: id integer and name string, which has a default value. You can create an instance, user = User(id=\"123\") You can also define models that include other models, allowing for complex data structures: from typing import List class Item(BaseModel): name: str price: float class Order(BaseModel): items: List[Item] total_price: float order = Order(items=[{\"name\": \"Burger\", \"price\": 5.99}, {\"name\": \"Fries\", \"price\": 2.99}], total_price=8.98) print(order) ","date":"2024-04-26","objectID":"/20240426_pydantic/:0:0","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Validators Pydantic provides a versatile decorator called validator, which enables you to impose custom validation rules on model fields. These validators extend beyond simple type validation and allow you to enforce additional checks. Here’s how you can define and utilize a custom validator: from pydantic import BaseModel, validator class Person(BaseModel): name: str age: int @validator('age') def check_age(cls, value): if value \u003c 18: raise ValueError('Age must be at least 18') return value # This will raise an error because the age is below 18 try: Person(name=\"Charlie\", age=17) except Exception as e: print(e) In this example, the custom validator ensures that the age provided is at least 18 years. Custom validators can target individual fields, multiple fields, or the entire model, making them invaluable for enforcing complex validation logic or cross-field constraints. ","date":"2024-04-26","objectID":"/20240426_pydantic/:1:0","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Built-in Validators Pydantic models leverage Python type annotations to enforce data types. Alongside the fundamental types like str, int, float, bool, Pydantic supports complex data types such as List, Dict, Union, and Optional, among others. These annotations are the first level of validation: from pydantic import BaseModel from typing import List, Optional class User(BaseModel): name: str age: int tags: Optional[List[str]] = None In this example, name must be a string, age an integer, and tags is an optional list of strings. ","date":"2024-04-26","objectID":"/20240426_pydantic/:1:1","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Field Validation For more detailed validation, Pydantic’s Field function can be used to specify additional constraints: from pydantic import BaseModel, Field class User(BaseModel): id: int name: str email: str = Field(..., description=\"The email address of the user\") age: int = Field(..., gt=0, description=\"The age of the user\") # Usage user_data = {\"id\": 1, \"name\": \"John\", \"email\": \"john@example.com\", \"age\": 30} user = User(**user_data) In this example: id, name, email, and age represents fields in the User model. id and name are required fields because they don’t have a default value. email and age have default values specified using the Field class. For email, ... indicates that it’s required, and a description is provided. For age, ... indicates that it’s required, and it must be greater than zero (gt=0). By using Field, you can define additional constraints such as minimum and maximum values, regular expressions for string fields, custom validation functions, etc., to ensure that your data meets specific criteria. age: int = Field(..., gt=0, description=\"The age of the user\") For instance, you can specifies that the age must be greater than 0. ","date":"2024-04-26","objectID":"/20240426_pydantic/:1:2","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Root Validators For validation that involves multiple fields, you can use root validators. These are applied to the whole model instead of individual fields: from pydantic import BaseModel, root_validator class Account(BaseModel): username: str password1: str password2: str @root_validator def passwords_match(cls, values): password1, password2 = values.get('password1'), values.get('password2') if password1 and password2 and password1 != password2: raise ValueError('Passwords do not match') return values Root validators have access to all field values of the model, making them ideal for validations that depend on multiple fields. ","date":"2024-04-26","objectID":"/20240426_pydantic/:1:3","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Pre-Validators and Post-Validators Pre-validators: A pre-validator in Pydantic is used to preprocess or transform the data before it undergoes the main validation process. This is particularly useful when you need to adjust or prepare the incoming data so it can be successfully validated. For instance, you might want to strip whitespace from a string, convert data types, or decompose compound fields into simpler components before validation. from pydantic import BaseModel, validator class TrimmedStringModel(BaseModel): text: str @validator('text', pre=True) def strip_whitespace(cls, value): return value.strip() Post-validator Post-validators are used to validate or transform data after the main validation process. They are useful when certain validations depend on multiple fields or when you need to enforce complex constraints that are not covered by basic type annotations. Post-validators are also defined using the @validator decorator but without specifying pre=True. ","date":"2024-04-26","objectID":"/20240426_pydantic/:1:4","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Json Serialization It is really simple to convert Pydantic models to or from JSON. For example, user_json = user.json() You can convert your model instance to JSON file as above. Or you can make a dictionary by user.dict() Conversely, json_str = '{\"name\": \"Han\", \"account\": 1234}' User.parse_raw(json_str) ","date":"2024-04-26","objectID":"/20240426_pydantic/:2:0","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Pydantic for Config Pydantic can also be used for settings management by loading configuration from environment variables: from pydantic_settings import BaseSettings from pydantic.types import SecretStr class DatabaseSettings(BaseSettings): api_key: str database_password: str my_database_settings = DatabaseSettings(_env_file=\".env\") print(my_database_settings.api_key) This feature is particularly useful for 12-factor apps that require configuration through the environment for different deployment environments. Pydantic provides a powerful system for data validation, allowing you to enforce type constraints and custom validation rules on your data models. This capability ensures that the data your application works with is correct and consistent, reducing runtime errors and simplifying data handling. Let’s explore more about validation in Pydantic, including built-in validators and how to write custom validation functions. ","date":"2024-04-26","objectID":"/20240426_pydantic/:3:0","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"Pydantic SecretStr Pydantic’s SecretStr is a special data type designed to handle sensitive information, such as passwords or secret tokens, in a more secure manner. This type is part of Pydantic’s data types that provide tools for sensitive data, ensuring that such information isn’t accidentally printed or logged, which could lead to security vulnerabilities. from pydantic import BaseModel, SecretStr class User(BaseModel): username: str password: SecretStr # Usage user_data = {\"username\": \"john_doe\", \"password\": \"secretpassword\"} user = User(**user_data) print(user) # Output: User username='john_doe' password=SecretStr('********') from pydantic import BaseModel, SecretBytes class EncryptedData(BaseModel): data: SecretBytes # Usage encrypted_data = {\"data\": b\"encrypted binary data\"} data_object = EncryptedData(**encrypted_data) print(data_object) # Output: EncryptedData data=SecretBytes('********') from pydantic_settings import BaseSettings from pydantic.types import SecretStr class DatabaseSettings(BaseSettings): api_key: SecretStr database_password: SecretStr my_database_settings = DatabaseSettings(_env_file=\".env\") print(my_database_settings.api_key) ","date":"2024-04-26","objectID":"/20240426_pydantic/:3:1","tags":["Python","Pydantic"],"title":"Data validation with Pydantic!","uri":"/20240426_pydantic/"},{"categories":["Python"],"content":"A tutorial for Enum","date":"2024-04-26","objectID":"/20240426_enum/","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Enum is a way that Python enumerate variables. The enum module allows for the creation of enumerated constants—unique, immutable data types that are useful for representing a fixed set of values. These values, which are usually related by their context, are known as enumeration members. Enum provides… Uniqueness - Each member of an Enum is unique within its definition, meaning no two members can have the same value. Attempting to define two members with the same value will result in an error unless you explicitly allow aliases. Immutability - Enum members are immutable. Once the Enum class is defined, you cannot change the members or their values. Iterability and Comparability - Enum classes support iteration over their members and can be compared using identity and equality checks. Accessing Members - You can access enumeration members by their names or values: Auto - If you want to automatically assign values to enum members, you can use the auto() function from the same module: from enum import Enum class State(Enum): PLAYING=0 PAUSED=1 GAME_OVER=2 If we just want to make sure them to be unique and automatically assigned, then use auto() from enum import Enum, auto class State(Enum): PLAYING=auto() PAUSED=auto() GAME_OVER=auto() print(State.PLAYING) print(State.PLAYING.value) Or simply, from enum import Enum, auto class State(Enum): PLAYING, PAUSED, GAME_OVER=range(3) print(State.PLAYING) print(State.PLAYING.value) However, this hard codes numbers, which can create an issue in the future. ","date":"2024-04-26","objectID":"/20240426_enum/:0:0","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Iterating over Enum Members You can iterate over the members of an enum: for state in State: print(state) ","date":"2024-04-26","objectID":"/20240426_enum/:0:1","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Comparison of Enum Members Enum members are singleton objects, so comparison is possible by identity: if State.PLAYING is State.PLAYING: print(\"RED is indeed RED\") ","date":"2024-04-26","objectID":"/20240426_enum/:0:2","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Using Enum as a Type Hint Enums can be used as type hints, enhancing code readability and correctness: def paint(color: Color): print(f\"Painting with {color.name}\") ","date":"2024-04-26","objectID":"/20240426_enum/:0:3","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Extending Enums: IntEnum and StrEnum For enums where the members are specifically integers or strings, you can inherit from IntEnum or StrEnum for additional benefits, like being able to compare members to integers or strings directly. from enum import IntEnum class Priority(IntEnum): LOW = 1 MEDIUM = 2 HIGH = 3 # Direct comparison with integers if Priority.LOW \u003c Priority.HIGH: print(\"Low priority is less than high\") ","date":"2024-04-26","objectID":"/20240426_enum/:0:4","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Unique Constraint To ensure that all enum values are unique, you can use the @unique decorator: from enum import Enum, unique @unique class StatusCode(Enum): OK = 200 NOT_FOUND = 404 ERROR = 500 Using @unique will raise a ValueError if any duplicate values are detected. ","date":"2024-04-26","objectID":"/20240426_enum/:0:5","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"Conclusion Enums in Python are useful for defining sets of named constants that are related and have a fixed set of members. They improve code readability, prevent errors related to using incorrect literal values, and can simplify type checking and validation in your programs. ","date":"2024-04-26","objectID":"/20240426_enum/:0:6","tags":["Python","Enum"],"title":"Enumerate variables with Enum!","uri":"/20240426_enum/"},{"categories":["Python"],"content":"A tutorial for Pytest","date":"2024-04-26","objectID":"/20240426_unit-tests/","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/20240426_unit-tests/"},{"categories":["Python"],"content":"Unit testing involves testing individual components of software in isolation to ensure they function correctly. Automated frameworks facilitate this process, which is integral to ensuring that new changes do not disrupt existing functionality. Unit tests also serve as practical documentation and encourage better software design. This testing method boosts development speed and confidence by confirming component reliability before integration. Early bug detection through unit testing also helps minimize future repair costs and efforts. pytest Pytest is one of the best tools that you can use to boost your testing productivity for Python codes. ","date":"2024-04-26","objectID":"/20240426_unit-tests/:0:0","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/20240426_unit-tests/"},{"categories":["Python"],"content":"Install pip install pytest pip install pytest-cov pytest --cov: this returns a coverage of test functions coverage html: log test results in html format ","date":"2024-04-26","objectID":"/20240426_unit-tests/:1:0","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/20240426_unit-tests/"},{"categories":["Python"],"content":"Example pytest is a libarary for testing. You can run your unit test code by pytest test_function.py If you wanna create a directory with several testing files, then just create __init__.py and put it inside the test dir. Then, just run pytest test_dir from calc import square import pytest def square(n): return n*n # This is a convention test_{func} def test_negative(): assert square(-2)==4 assert square(-3)==9 def test_positive(): assert square(2)==4 assert square(3)==9 def test_zero(): assert square(0)==0 def test_str(): # Write down what I expect to get # If it successfully raised the error that I expected # Then, it will pass the test with pytest.raises(TypeError): square(\"cat\") def main(): x = int(input(\"What's x? \")) print(\"x squared is\", square(x)) if __name__==\"__main__\": main() If you want to intentially raise an error, then you can do it by pytest.raises(\"SomeErrorType\") Note that you can write a warning message like assert x == \u003ccond\u003e, \u003cMSG\u003e ","date":"2024-04-26","objectID":"/20240426_unit-tests/:2:0","tags":["Python","Pytest"],"title":"Unit Test with Pytest","uri":"/20240426_unit-tests/"},{"categories":["Programming"],"content":"A gentle introduction to bash scripting","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Let’s create our first simple shell script #!/bin/sh # This is a comment! echo Hello World # This is a comment, too! The first line tells Unix that the file is to be executed by /bin/sh. This is the standard location of the Bourne shell on just about every Unix system. If you’re using GNU/Linux, /bin/sh is normally a symbolic link to bash (or, more recently, dash). The second line begins with a special symbol: #. This marks the line as a comment, and it is ignored completely by the shell. The only exception is when the very first line of the file starts with #! (shebang) - as ours does. This is a special directive which Unix treats specially. It means that even if you are using csh, ksh, or anything else as your interactive shell, that what follows should be interpreted by the Bourne shell. Similarly, a Perl script may start with the line #!/usr/bin/perl to tell your interactive shell that the program which follows should be executed by perl. For Bourne shell programming, we shall stick to #!/bin/sh. The third line runs a command: echo, with two parameters, or arguments - the first is \"Hello\"; the second is \"World\". Note that echo will automatically put a single space between its parameters. To make it executable, run chmod +rx \u003cfilename\u003e ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:0:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Variables Let’s look back at our first Hello World example. This could be done using variables. Note that there must be no spaces around the “=” sign: VAR=value works; VAR = value doesn’t work. In the first case, the shell sees the “=” symbol and treats the command as a variable assignment. In the second case, the shell assumes that VAR must be the name of a command and tries to execute it. #!/bin/sh MY_MESSAGE=\"Hello World\" echo $MY_MESSAGE This assigns the string “Hello World” to the variable MY_MESSAGE then echoes out the value of the variable. Note that we need the quotes around the string Hello World. Whereas we could get away with echo Hello World because echo will take any number of parameters, a variable can only hold one value, so a string with spaces must be quoted so that the shell knows to treat it all as one. Otherwise, the shell will try to execute the command World after assigning MY_MESSAGE=Hello The shell does not care about types of variables; they may store strings, integers, real numbers - anything you like. We can interactively set variable names using the read command; the following script asks you for your name then greets you personally #!/bin/sh echo What is your name? read MY_NAME echo \"Hello $MY_NAME - hope you're well.\" ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:1:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Escape Characters Certain characters are significant to the shell; for example, that the use of double quotes (\") characters affect how spaces and TAB characters are treated, for example: $ echo Hello World Hello World $ echo \"Hello World\" Hello World So how do we display: Hello \"World\" ? $ echo \"Hello \\\"World\\\"\" The first and last \" characters wrap the whole lot into one parameter passed to echo so that the spacing between the two words is kept as is. But the code: $ echo \"Hello \" World \"\" would be interpreted as three parameters: “Hello \" World \"” So the output would be Hello World Note that we lose the quotes entirely. This is because the first and second quotes mark off the Hello and following spaces; the second argument is an unquoted “World” and the third argument is the empty string; “”. ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:2:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Loop ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:3:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"For Loop #!/bin/sh for i in 1 2 3 4 5 do echo \"Looping ... number $i\" done #!/bin/sh for i in hello 1 * 2 goodbye do echo \"Looping ... i is set to $i\" done The output of the above code is Looping ... i is set to hello Looping ... i is set to 1 Looping ... i is set to (name of first file in current directory) ... etc ... Looping ... i is set to (name of last file in current directory) Looping ... i is set to 2 Looping ... i is set to goodbye This is well worth trying. Make sure that you understand what is happening here. Try it without the * and grasp the idea, then re-read the Wildcards section and try it again with the * in place. Try it also in different directories, and with the * surrounded by double quotes, and try it preceded by a backslash (\\*) ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:3:1","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"While Loop #!/bin/sh INPUT_STRING=hello while [ \"$INPUT_STRING\" != \"bye\" ] do echo \"Please type something in (bye to quit)\" read INPUT_STRING echo \"You typed: $INPUT_STRING\" done #!/bin/sh while : do echo \"Please type something in (^C to quit)\" read INPUT_STRING echo \"You typed: $INPUT_STRING\" done The colon (:) always evaluates to true; whilst using this can be necessary sometimes, it is often preferable to use a real exit condition. Compare quitting the above loop with the one below; see which is the more elegant. Also think of some situations in which each one would be more useful than the other: #!/bin/sh while read input_text do case $input_text in hello) echo English ;; howdy) echo American ;; gday) echo Australian ;; bonjour) echo French ;; \"guten tag\") echo German ;; *) echo Unknown Language: $input_text ;; esac done \u003c myfile.txt This reads the file “myfile.txt”, one line at a time, into the variable “$input_text”. The case statement then checks the value of $input_text. If the word that was read from myfile.txt was “hello” then it echoes the word “English”. If it was “gday” then it will echo Australian. If the word (or words) read from a line of myfile.txt don’t match any of the provided patterns, then the catch-all “*” default will display the message “Unknown Language: $input_text” - where of course “$input_text” is the value of the line that it read in from myfile.txt. A handy Bash (but not Bourne Shell) tip I learned recently from the Linux From Scratch project is: mkdir rc{0,1,2,3,4,5,6,S}.d instead of the more cumbersome: for runlevel in 0 1 2 3 4 5 6 S do mkdir rc${runlevel}.d done And ls can be done recursively, too: $ cd / $ ls -ld {,usr,usr/local}/{bin,sbin,lib} drwxr-xr-x 2 root root 4096 Oct 26 01:00 /bin drwxr-xr-x 6 root root 4096 Jan 16 17:09 /lib drwxr-xr-x 2 root root 4096 Oct 27 00:02 /sbin drwxr-xr-x 2 root root 40960 Jan 16 19:35 usr/bin drwxr-xr-x 83 root root 49152 Jan 16 17:23 usr/lib drwxr-xr-x 2 root root 4096 Jan 16 22:22 usr/local/bin drwxr-xr-x 3 root root 4096 Jan 16 19:17 usr/local/lib drwxr-xr-x 2 root root 4096 Dec 28 00:44 usr/local/sbin drwxr-xr-x 2 root root 8192 Dec 27 02:10 usr/sbin ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:3:2","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Test Test is used by virtually every shell script written. It may not seem that way, because test is not often called directly. test is more frequently called as [. [ is a symbolic link to test, just to make shell programs more readable. It is also normally a shell builtin (which means that the shell itself will interpret [ as meaning test, even if your Unix environment is set up differently): $ type [ [ is a shell builtin $ which [ /usr/bin/[ $ ls -l /usr/bin/[ lrwxrwxrwx 1 root root 4 Mar 27 2000 /usr/bin/[ -\u003e test $ ls -l /usr/bin/test -rwxr-xr-x 1 root root 35368 Mar 27 2000 /usr/bin/test This means that ‘[’ is actually a program, just like ls and other programs, so it must be surrounded by spaces: if [$foo = \"bar\" ] will not work; it is interpreted as if test$foo = \"bar\" ], which is a ‘]’ without a beginning ‘[’. Put spaces around all your operators. I’ve highlighted the mandatory spaces with the word ‘SPACE’ . Note: Some shells also accept “==” for string comparison; this is not portable, a single “=” should be used for strings, or “-eq” for integers. Test is a simple but powerful comparison utility. For full details, run man test on your system, but here are some usages and typical examples. Test is most often invoked indirectly via the if and while statements. It is also the reason you will come into difficulties if you create a program called test and try to run it, as this shell builtin will be called instead of your program! The syntax for if...then...else... is: if [ ... ] then # if-code else # else-code fi Also, be aware of the syntax - the “if [ ... ]” and the “then” commands must be on different lines. Alternatively, the semicolon “;” can separate them: if [ ... ]; then # do something fi You can also use the elif, like this: if [ something ]; then echo \"Something\" elif [ something_else ]; then echo \"Something else\" else echo \"None of the above\" fi This will echo \"Something\" if the [ something ] test succeeds, otherwise it will test [ something_else ], and echo \"Something else\" if that succeeds. If all else fails, it will echo \"None of the above\". ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:4:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Case The case statsement saves going through a whole set of if ... then ... else statements. Its syntax is really simple: #!/bin/sh echo \"Please talk to me ...\" while : do read INPUT_STRING case $INPUT_STRING in hello) echo \"Hello yourself!\" ;; bye) echo \"See you again!\" break ;; *) echo \"Sorry I don't understand\" ;; esac done ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:5:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Variables 2 The first set of variables we will look at are $0 ... $9 and $#. The variable $0 is the basename of the program as it was called. $1...$9 are the first 9 additional parameters the script was called with. The variable $@ is all parameters. The variable $* is similar, but does not preserve any whitespace and quoting, so “File with spaces” becomes “File”, “with”, and “spaces”. $# is the number of parameters the script was called with. #!/bin/sh echo \"I was called with $# parameters\" echo \"My name is $0\" echo \"My first parameter is $1\" The othere two main variables set are $$ and $!. These are both process numbers. The $$ variable is the PID of the currently running shell. This can be useful for creating temporary files, such as /tmp/my-script.$$ which is useful if many instances of the script could be run at the same time, and they all need their own temporary files. The $! variable is the PID of the last run background processd. This is useful to keep track of the process as it gets on with its job. Another interesting vardfiable is IFS. This is the Interfal Field Separator. The default value is SPACE TAB NEWLINE, but if you are changing it, it’s easier to take a copy as shown: #!/bin/sh old_IFS=\"$IFS\" IFS=: # Set IFS as colon echo \"Please input some data separated by colons\" read x y z IFS=$old_IFS echo \"x is $x y is $y z is $z\" ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:6:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Functions #!/bin/sh add_a_user() { USER=$1 PASSWORD=$2 COMMENTS=$@ echo \"Adding user $USER\" echo useradd -c \"$COMMENTS\" $USER echo passwd $USER $PASSWORD } ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:7:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Programming"],"content":"Reference shellscript ","date":"2024-04-21","objectID":"/20240421_the-shell-script-tutorial/:8:0","tags":["Python","bash","shell script","linux"],"title":"Bash script tutorial","uri":"/20240421_the-shell-script-tutorial/"},{"categories":["Linux"],"content":"You should use Linux!","date":"2024-04-21","objectID":"/20240421_why-linux/","tags":["Linux","Minimalism"],"title":"Minimalism Through Linux","uri":"/20240421_why-linux/"},{"categories":["Linux"],"content":"Linux, A Path to Digital Simplicity In an age dominated by digital clutter and overwhelming software choices, the minimalist philosophy stands out as a beacon for those seeking simplicity and efficiency. This approach not only applies to physical possessions but extends into the digital realm, where Linux has become a preferred tool for minimalists. Linux, an open-source operating system, embodies the principles of minimalism by offering users control over their digital environments. Unlike mainstream operating systems that often come loaded with non-essential features and bloatware, Linux allows users to select only the components they need, creating a lean and efficient system. The minimalist appeal of Linux is evident in its customizable nature. Users can choose from a variety of distributions, each tailored for different needs. For instance, distributions like Arch Linux provide barebone setup that users can expand as needed. This customization extends to the user interface, where options range from feature-rich desktop environments to more austere ones like i3wm or DWM, which use fewer resources and maintain a clean, unobtrusive design. Furthermore, Linux’s robust command-line interface is a minimalist’s dream. It enables users to perform tasks efficiently without the graphical overhead, which is particularly appealing to those who favor functionality and speed over visual elements. Moreover, Linux supports the concept of free software, which resonates with minimalists’ preference for authenticity and freedom from commercial constraints. Users are free to modify, improve, and redistribute their software in ways that proprietary systems do not allow. Linux offers a compelling choice for anyone looking to embrace a minimalist digital lifestyle. It provides the tools to create a personalized and simple digital environment, encourages the efficient use of resources, and upholds values of freedom and simplicity. For those seeking to reduce their digital footprint while maximizing functionality, Linux proves that less can indeed be more. ","date":"2024-04-21","objectID":"/20240421_why-linux/:0:1","tags":["Linux","Minimalism"],"title":"Minimalism Through Linux","uri":"/20240421_why-linux/"},{"categories":["Python"],"content":"Guide to keep sensitive data in Python","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"An app’s config is everything that is likely to vary between deploys (staging, production, developer environments, etc). This includes: Resource handles to the database, Memcached, and other backing services Credentials to external services such as Amazon S3 or Twitter Per-deploy values such as the canonical hostname for the deploy Apps sometimes store config as constants in the code. This is a violation of twelve-factor, which requires strict separation of config from code. Config varies substantially across deploys, code does not. A litmus test for whether an app has all config correctly factored out of the code is whether the codebase could be made open source at any moment, without compromising any credentials. The twelve-factor app stores config in environment variables (often shortened to env vars or env). Env vars are easy to change between deploys without changing any code; unlike config files, there is little chance of them being checked into the code repo accidentally; and unlike custom config files, or other config mechanisms such as Java System Properties, they are a language- and OS-agnostic standard. In a twelve-factor app, env vars are granular controls, each fully orthogonal to other env vars. They are never grouped together as “environments”, but instead are independently managed for each deploy. This is a model that scales up smoothly as the app naturally expands into more deploys over its lifetime. ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:0:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"Environment Variable For example, you shouldn’t put information directly in your code. db_user = 'my_db_user' db_password = 'my_db_pass_123!' Let’s keep the sensitive information in .bash_profile as follows: export DB_USER=\"my_db_user\" export DB_PASS='my_db_pass_123!' Then, we can call them by import os db_user = os.environ.get('DB_USER') db_password = os.environ.get('DB_PASS') ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:1:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"dotenv ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:2:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"Introduction Python-dotenv reads key-value pairs from a .env file and can set them as environment variables. It helps in the development of applications following the 12-factor principles. ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:2:1","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"Basics Installation: pip install python-dotenv Create .env file in your project directory. Put the data (or variables) in the .env file. e.g, API_KEY=\"dafjei304aldkjf20akj\" To load your key, First, use load_dotenv() with os.getenv(\"[Your variable]\") e.g., API_KEY=os.getenv(\"API_KEY\") Make sure to update .gitignore to exclude the .env file. from dotenv import load_dotenv load_dotenv() API_KEY = os.getenv(\"API_KEY\") or \"\" ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:2:2","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"Reference The Twelve Factor App ","date":"2024-04-20","objectID":"/20240421_hide-sensitive-data/:3:0","tags":["Python","dotenv"],"title":"How to keep sensitive data in Python?","uri":"/20240421_hide-sensitive-data/"},{"categories":["Python"],"content":"Guide to understand type hint in Python.","date":"2024-04-20","objectID":"/20240421_type-hint/","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Type hinting is not mandatory, but it can make your code easier to understand and debug by Improved readability Better IDE support: IDEs and linters can use type hints to check your code for potential errors before runtime. While type hints can be simple classes like float or str , they can also be more complex. The typing module provides a vocabulary of more advanced type hints. ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:0","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Basics # This is how you declare the type of a variable age: int = 1 # You don't need to initialize a variable to annotate it a: int # Ok (no value at runtime until assigned) # Doing so can be useful in conditional branches child: bool if age \u003c 18: child = True else: child = False x: int = 1 x: float = 1.0 x: bool = True x: str = \"test\" x: bytes = b\"test\" # For collections on Python 3.9+, the type of the collection item is in brackets x: list[int] = [1] x: set[int] = {6, 7} # For mappings, we need the types of both keys and values x: dict[str, float] = {\"field\": 2.0} # Python 3.9+ # For tuples of fixed size, we specify the types of all the elements x: tuple[int, str, float] = (3, \"yes\", 7.5) # Python 3.9+ # For tuples of variable size, we use one type and ellipsis x: tuple[int, ...] = (1, 2, 3) # Python 3.9+ # On Python 3.8 and earlier, the name of the collection type is # capitalized, and the type is imported from the 'typing' module from typing import List, Set, Dict, Tuple x: List[int] = [1] x: Set[int] = {6, 7} x: Dict[str, float] = {\"field\": 2.0} x: Tuple[int, str, float] = (3, \"yes\", 7.5) x: Tuple[int, ...] = (1, 2, 3) ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:1","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Union Union is for multiple types def process_message(msg: Union[str, bytes, None]) -\u003e str: ... # On Python 3.10+, use the | operator when something could be one of a few types x: list[int | str] = [3, 5, \"test\", \"fun\"] # Python 3.10+ # On earlier versions, use Union x: list[Union[int, str]] = [3, 5, \"test\", \"fun\"] ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:2","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Optional # food can be either str or None. def eat_food(food: Optional[str]) -\u003e None: ... # Use Optional[X] for a value that could be None # Optional[X] is the same as X | None or Union[X, None] x: Optional[str] = \"something\" if some_condition() else None if x is not None: # Mypy understands x won't be None here because of the if-statement print(x.upper()) # If you know a value can never be None due to some logic that mypy doesn't # understand, use an assert assert x is not None print(x.upper()) ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:3","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Any Any is a special type hint in Python that indicates that a variable can be of any type. It essentially disables static type checking for that variable. It’s typically used when you want to explicitly indicate that a certain variable can have any type, or when dealing with dynamically typed code where the type of the variable cannot be easily inferred. While Any provides flexibility, it also sacrifices the benefits of static type checking, as type errors related to variables annotated as Any won’t be caught by type checkers. ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:4","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Functions: Callable Types Callable type hint can define types for callable functions. from typing import Callable Callable[[Parameter types, ...], return_types] Callable objects are functions, classes, and so on. Type [input types] and return types def on_some_event_happened(callback: Callable[[int, str, str], int]) -\u003e None: ... def do_this(a: int, b: str, c:str) -\u003e int: ... on_some_event_happened(do_this) # This is how you annotate a callable (function) value x: Callable[[int, float], float] = f def register(callback: Callable[[str], int]) -\u003e None: ... # A generator function that yields ints is secretly just a function that # returns an iterator of ints, so that's how we annotate it def gen(n: int) -\u003e Iterator[int]: i = 0 while i \u003c n: yield i i += 1 # You can of course split a function annotation over multiple lines def send_email(address: Union[str, list[str]], sender: str, cc: Optional[list[str]], bcc: Optional[list[str]], subject: str = '', body: Optional[list[str]] = None ) -\u003e bool: ... ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:5","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Classes class BankAccount: # The \"__init__\" method doesn't return anything, so it gets return # type \"None\" just like any other method that doesn't return anything def __init__(self, account_name: str, initial_balance: int = 0) -\u003e None: # mypy will infer the correct types for these instance variables # based on the types of the parameters. self.account_name = account_name self.balance = initial_balance # For instance methods, omit type for \"self\" def deposit(self, amount: int) -\u003e None: self.balance += amount def withdraw(self, amount: int) -\u003e None: self.balance -= amount # User-defined classes are valid as types in annotations account: BankAccount = BankAccount(\"Alice\", 400) def transfer(src: BankAccount, dst: BankAccount, amount: int) -\u003e None: src.withdraw(amount) dst.deposit(amount) ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:6","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Annotated Annotated in python allows developers to declare type of a reference and and also to provide additional information related to it. name = Annotated[str, \"first letter is capital\"] This tells that name is of type str and that name[0] is a capital letter. On its own Annotated does not do anything other than assigning extra information (metadata) to a reference. It is up to another code, which can be a library, framework or your own code, to interpret the metadata and make use of it. For example FastAPI uses Annotated for data validation: def read_items(q: Annotated[str, Query(max_length=50)]) Here the parameter q is of type str with a maximum length of 50. This information was communicated to FastAPI (or any other underlying library) using the Annotated keyword. Annotated[\u003ctype\u003e, \u003cmetadata\u003e] Here is an example of how you might use Annotated to add metadata to type annotations if you were doing range analysis: @dataclass class ValueRange: lo: int hi: int T1 = Annotated[int, ValueRange(-10, 5)] T2 = Annotated[T1, ValueRange(-20, 3)] ","date":"2024-04-20","objectID":"/20240421_type-hint/:0:7","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"TypeVar This is a special type for generic types. from typing import Sequence, TypeVar, Iterable T = TypeVar(\"T\") # `T` is typically used to represent a generic type variable def batch_iter(data: Sequence[T], size: int) -\u003e Iterable[Sequence[T]]: for i in range(0, len(data), size): yield data[i:i + size] Since the generic type is used, batch_iter function can take any type of Sequence type data. For instance, Sequence[int], Sequence[str], Sequence[Person] If we use bound, then we can restrict the generic type. For example, from typing import Sequence, TypeVar, Iterable, Union T = TypeVar(\"T\", bound=Union[int, str, bytes]) def batch_iter(data: Sequence[T], size: int) -\u003e Iterable[Sequence[T]]: for i in range(0, len(data), size): yield data[i:i + size] Thus, the following code will show an error as it takes a list of float numbers: batch_iter([1.1, 1.3, 2.5, 4.2, 5.5], 2) Note that in Python 3.12, generic type hint has been changed ","date":"2024-04-20","objectID":"/20240421_type-hint/:1:0","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":["Python"],"content":"Reference ArjanCodes Type hint cheat sheet ","date":"2024-04-20","objectID":"/20240421_type-hint/:2:0","tags":["Python","type hint"],"title":"Type hint in Python","uri":"/20240421_type-hint/"},{"categories":null,"content":"About Han","date":"2024-04-21","objectID":"/about/","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Han Cheol Moon Welcome to Han's XYZ, where you'll explore a world of ideas across every dimension of thought. I am a curiosity-driven Machine Learning Scientist, passionate about exploring the depths of data and algorithms. As an enthusiastic learner and code lover, I am always eager to expand my knowledge and embrace innovation, with a commitment to sharing my insights and discoveries with you. ","date":"2024-04-21","objectID":"/about/:0:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Research Interests Natural language processing (NLP) Robustness of NLP systems ","date":"2024-04-21","objectID":"/about/:1:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Professional Employments Samsung Electronics Staff ML/DL Engineer, Sept 2023 - Present ","date":"2024-04-21","objectID":"/about/:2:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Education Nanyang Technological University, 2019-2023 Ph.D. in Computer Science Yonsei University, 2016-2018 M.S. in Electrical \u0026 Electronic Engineering Chung-Ang University, 2009-2016 B.S. in Electrical \u0026 Electronic Engineering (Military Service, 2009-2012) ","date":"2024-04-21","objectID":"/about/:3:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Skills Programming Languages Python, C/C++, MATLAB, and Bash/Shell ML/DL Research PyTorch, Tensorflow, Scipy, Numpy, Scikit-learn, Pandas, and HuggingFace ML/DL Ops Git, Docker, Kubernetes, Jenkins, DVC, FastAPI, and LangChain Database SQLite and PostgreSQL with SQLAlchemy and Alembic WebDev HTML/CSS, Django, and Hugo WorkFlow I’ve used Microsoft and GNU/Linux systems (both Debian and Arch-based varieties) with a Vim-based setup for writing scripts and managing files in a tiling window manager (i3). I use Git for version control with Docker and schedule multiple tasks with TaskSpooler. I compile documents using $\\LaTeX$. Soft Skills Time Management, Teamwork, Problem-solving, Documentation, Engaging Presentation. ","date":"2024-04-21","objectID":"/about/:4:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Teaching ","date":"2024-04-21","objectID":"/about/:5:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Teaching Assistant DeepNLP, 2019-2020 Nanyang Technological University, Singapore Digital Logic Circuits, Spring 2018 Yonsei University, Korea Multimedia Signal Processing, Fall 2017 Yonsei University, Korea Signals and Systems, Fall 2016 Yonsei University, Korea ","date":"2024-04-21","objectID":"/about/:5:1","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Honors and Award Singapore International Graduate Award (SINGA), 2019-2023 Top of Class Scholarship, Chung-Ang University, 2015 Dean’s List, Chung-Ang University, 2013-2014 ","date":"2024-04-21","objectID":"/about/:6:0","tags":null,"title":"About Han","uri":"/about/"},{"categories":null,"content":"Service and leadership Military Service, 2009-2012 Republic of Korea Army, Capital Defense Command ‘SHIELD’, 1st Security Group Guarding presidential residence Sergeant ","date":"2024-04-21","objectID":"/about/:7:0","tags":null,"title":"About Han","uri":"/about/"}]