<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>LS-SVM - Tag - Han&#39;s XYZ</title>
        <link>http://localhost:1313/tags/ls-svm/</link>
        <description>LS-SVM - Tag - Han&#39;s XYZ</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>tabularasa8931@gmail.com (Han)</managingEditor>
            <webMaster>tabularasa8931@gmail.com (Han)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 31 Aug 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/tags/ls-svm/" rel="self" type="application/rss+xml" /><item>
    <title>Introduction to SVM Part 2. LS-SVM</title>
    <link>http://localhost:1313/20240825_svm2/</link>
    <pubDate>Sat, 31 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20240825_svm2/</guid>
    <description><![CDATA[Introduction to Least-Square SVM Introduction Least Squares Support Vector Machine (LS-SVM) is a modified version of the traditional Support Vector Machine (SVM) that simplifies the quadratic optimization problem by using a least squares cost function. LS-SVM transforms the quadratic programming problem in classical SVM into a set of linear equations, which are easier and faster to solve.
Optimization Problem (Primal Problem) \begin{align*} &amp;\min_{w, b, e} \frac{1}{2} \lVert w\rVert^2 + \frac{\gamma}{2} \sum_{i=1}^N e_i^2,\\ &amp;\text{subject to } y_i (w^T \phi(x_i) + b) = 1 - e_i, \ \forall i \end{align*} where:]]></description>
</item>
</channel>
</rss>
