<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Weighted Least Square - Tag - Han&#39;s XYZ</title>
        <link>http://localhost:1313/tags/weighted-least-square/</link>
        <description>Weighted Least Square - Tag - Han&#39;s XYZ</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>tabularasa8931@gmail.com (Han)</managingEditor>
            <webMaster>tabularasa8931@gmail.com (Han)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 11 Aug 2024 00:00:00 &#43;0000</lastBuildDate><atom:link href="http://localhost:1313/tags/weighted-least-square/" rel="self" type="application/rss+xml" /><item>
    <title>Getting Started with Regression (Part 2)</title>
    <link>http://localhost:1313/20240811_regression2/</link>
    <pubDate>Sun, 11 Aug 2024 00:00:00 &#43;0000</pubDate>
    <author>Han</author>
    <guid>http://localhost:1313/20240811_regression2/</guid>
    <description><![CDATA[An Introductory Guide (Part 2) Understanding Ridge Regression In machine learning, one of the key challenges is finding the right balance between underfitting and overfitting a model.
Overfitting occurs when a model is too complex and captures not only the underlying patterns in the training data but also the noise. This results in a model that performs well on the training data but poorly on new, unseen data.
Underfitting, on the other hand, happens when a model is too simple to capture the underlying patterns in the data, leading to poor performance both on the training data and on new data.]]></description>
</item>
</channel>
</rss>
